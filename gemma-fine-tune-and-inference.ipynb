{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05c66951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (4.52.0.dev0)\n",
      "Requirement already satisfied: datasets in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (2.21.0)\n",
      "Requirement already satisfied: peft in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (0.14.0)\n",
      "Requirement already satisfied: accelerate in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (1.6.0)\n",
      "Requirement already satisfied: bitsandbytes in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (0.45.5)\n",
      "Requirement already satisfied: huggingface_hub in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (0.30.2)\n",
      "Requirement already satisfied: filelock in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: psutil in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from peft) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from huggingface_hub) (4.13.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: networkx in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets peft accelerate bitsandbytes huggingface_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1307276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (4.52.0.dev0)\n",
      "Requirement already satisfied: filelock in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from requests->transformers) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "152ac3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.52.0.dev0\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: bert-score, kvpress, peft, sentence-transformers, trl, unsloth_zoo\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaa9e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fine tuning gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fee0f1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"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\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89dc6567",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 745,472 || all params: 1,000,631,424 || trainable%: 0.0745\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "437eadeb2c084fc6968ffa7e7394eb1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/851 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_18543/1334388288.py:98: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "It is strongly recommended to train Gemma3 models with the `eager` attention implementation instead of `sdpa`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1278' max='1278' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1278/1278 05:59, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.674800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.329500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.844000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.345500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.863900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.410400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.932700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.432700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.987700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.446600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.388800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.381800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.378800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.377100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.375700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.374500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.373300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.372200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.370900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.369400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.367900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.366100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.364100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.361700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.359300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.356800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.353500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.349900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.345600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.340500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.334800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.329000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.320600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.313700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.306200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.298800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.294400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.287700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.283300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.280600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.280100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.279300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.279500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.278200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.277700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.277600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.277300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.277300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.277100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.276900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.277000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.276600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.277000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.276400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.276600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.276800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.276300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.276300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.276200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.276300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.276400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.276300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.276500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.276300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.276300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.276100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.276200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.276100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.276200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.276000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.276300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.276200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.276000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.276000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.276100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.276000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.276000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.276000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.276200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.276000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.276000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.276200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.275900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.276000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.276200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.276000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.276000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.275900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.276000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.275900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.276100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.276000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.275900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.276100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.276000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>0.275900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.276200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.276000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.275900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>0.276000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.276000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>0.275900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.275900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.275800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.276000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>0.275900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.275800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>0.276100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.275900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>0.275800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.276100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>0.275800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.275900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.275900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.275900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>0.275900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.275900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>0.275900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.275900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>0.275800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>0.275800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>0.275800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.275900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.275900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>0.275800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>0.276100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1278, training_loss=0.4797283780406898, metrics={'train_runtime': 361.343, 'train_samples_per_second': 7.065, 'train_steps_per_second': 3.537, 'total_flos': 5479317016805376.0, 'train_loss': 0.4797283780406898, 'epoch': 3.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# CONFIG\n",
    "model_id = \"google/gemma-3-1b-it\"\n",
    "dataset_id = \"ishani29/mahakumbh-news-summarization\"\n",
    "split = \"train\"\n",
    "max_input_length = 512\n",
    "max_target_length = 150\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",       # 🚀 Handles device placement correctly\n",
    "    torch_dtype=\"auto\"       # or torch.float16 if you want faster inference\n",
    ")\n",
    "\n",
    "# Apply PEFT LoRA\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # target attention modules (adjust for Gemma)\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(dataset_id, split=split)\n",
    "\n",
    "# Preprocess data\n",
    "# def preprocess_function(example):\n",
    "#     prompt = f\"Summarize the following news article:\\n{example['text']}\\nSummary:\"\n",
    "#     inputs = tokenizer(prompt, max_length=max_input_length, truncation=True)\n",
    "#     targets = tokenizer(example[\"summary\"], max_length=max_target_length, truncation=True)\n",
    "#     inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "#     return inputs\n",
    "def preprocess_function(example):\n",
    "    prompt = prompt = (\n",
    "    \"You are a helpful assistant trained to summarize Indian news articles concisely in less than or equal to 100 words.\\n\\n\"\n",
    "    \"Article:\\n{example['text']}\\n\\n\"\n",
    "    \"Write a clear, factual, and concise summary and ensure that no noisy statements are added:\"\n",
    ")\n",
    "\n",
    "    \n",
    "    # Tokenize inputs and targets with padding\n",
    "    model_inputs = tokenizer(\n",
    "        prompt,\n",
    "        max_length=max_input_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            example[\"summary\"],\n",
    "            max_length=max_target_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return {k: v.squeeze(0) for k, v in model_inputs.items()}\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gemma-lora-summary\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    fp16=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\",  # disable wandb\n",
    "    logging_steps=10,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"ishani29/gemma-summary-lora\",\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c5cc2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ishani29/gemma-summary-lora/commit/13b21d7ae414175a3dc393625f0a716044e2801f', commit_message='End of training', commit_description='', oid='13b21d7ae414175a3dc393625f0a716044e2801f', pr_url=None, repo_url=RepoUrl('https://huggingface.co/ishani29/gemma-summary-lora', endpoint='https://huggingface.co', repo_type='model', repo_id='ishani29/gemma-summary-lora'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af622ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "448cc1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /mnt/Data/sarmistha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /mnt/Data/sarmistha/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /mnt/Data/sarmistha/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Computing BLEU...\n",
      "🔍 Computing ROUGE...\n",
      "🔍 Computing METEOR...\n",
      "🔍 Computing BERTScore...\n",
      "\n",
      "📊 Evaluation Summary:\n",
      "BLEU: 0.0545\n",
      "ROUGE-1: 0.2844\n",
      "ROUGE-2: 0.0944\n",
      "ROUGE-L: 0.1818\n",
      "METEOR: 0.2701\n",
      "\n",
      "✅ Evaluation metrics saved to: 'mahakumbh_eval_metrics_gemmaaaa.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# ----- CONFIG -----\n",
    "csv_input_path = \"mahakumbh_test_predictions_gemmaaaa.csv\"  # Change if your CSV has a different name\n",
    "eval_output_path = \"mahakumbh_eval_metrics_gemmaaaa.csv\"\n",
    "model_id = \"ishani29/gemma-summary-lora\"  # Replace with your Gemma model checkpoint ID if calculating perplexity\n",
    "\n",
    "# ----- LOAD DATA -----\n",
    "df = pd.read_csv(csv_input_path)\n",
    "references = df[\"reference_summary\"].tolist()\n",
    "predictions = df[\"generated_summary\"].tolist()\n",
    "articles = df[\"article\"].tolist()\n",
    "\n",
    "# ----- EVALUATION METRICS -----\n",
    "bleu = evaluate.load(\"sacrebleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "meteor = evaluate.load(\"meteor\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "print(\"🔍 Computing BLEU...\")\n",
    "bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "\n",
    "print(\"🔍 Computing ROUGE...\")\n",
    "rouge_score = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"🔍 Computing METEOR...\")\n",
    "meteor_score = meteor.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"🔍 Computing BERTScore...\")\n",
    "# bert_score = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "\n",
    "# # ----- (OPTIONAL) PERPLEXITY -----\n",
    "# try:\n",
    "#     print(\"🔍 Calculating Perplexity (optional)...\")\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "#     model = AutoModelForSeq2SeqLM.from_pretrained(model_id).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     model.eval()\n",
    "\n",
    "#     perplexities = []\n",
    "#     for article, reference in tqdm(zip(articles, references), total=len(articles), desc=\"Calculating Perplexity\"):\n",
    "#         inputs = tokenizer(article, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n",
    "#         labels = tokenizer(reference, return_tensors=\"pt\", truncation=True, max_length=150).input_ids.to(model.device)\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             loss = model(input_ids=inputs.input_ids, labels=labels).loss\n",
    "#             perplexities.append(torch.exp(loss).item())\n",
    "\n",
    "#     df[\"perplexity\"] = perplexities\n",
    "#     avg_perplexity = sum(perplexities) / len(perplexities)\n",
    "# except Exception as e:\n",
    "#     print(f\"⚠️ Perplexity skipped due to error: {e}\")\n",
    "#     avg_perplexity = None\n",
    "\n",
    "# ----- SAVE METRICS -----\n",
    "metrics = {\n",
    "    \"BLEU\": bleu_score[\"score\"] / 100,\n",
    "    \"ROUGE-1\": rouge_score[\"rouge1\"],\n",
    "    \"ROUGE-2\": rouge_score[\"rouge2\"],\n",
    "    \"ROUGE-L\": rouge_score[\"rougeL\"],\n",
    "    \"METEOR\": meteor_score[\"meteor\"],\n",
    "#     \"BERTScore_F1\": sum(bert_score[\"f1\"]) / len(bert_score[\"f1\"]),\n",
    "#     \"Avg Perplexity\": avg_perplexity if avg_perplexity is not None else \"N/A\"\n",
    "}\n",
    "\n",
    "pd.DataFrame([metrics]).to_csv(eval_output_path, index=False)\n",
    "\n",
    "# ----- PRINT METRICS -----\n",
    "print(\"\\n📊 Evaluation Summary:\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
    "print(f\"\\n✅ Evaluation metrics saved to: '{eval_output_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cd4e997",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (4.52.0.dev0)\n",
      "Requirement already satisfied: datasets in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (2.21.0)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: bert_score in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (0.3.13)\n",
      "Requirement already satisfied: nltk in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (3.9.1)\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
      "Requirement already satisfied: filelock in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: torch>=1.0.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from bert_score) (2.5.0)\n",
      "Requirement already satisfied: matplotlib in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from bert_score) (3.10.0)\n",
      "Requirement already satisfied: click in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
      "Collecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting tabulate>=0.8.9 (from sacrebleu)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting colorama (from sacrebleu)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: lxml in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from sacrebleu) (5.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: networkx in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (1.13.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from matplotlib->bert_score) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from matplotlib->bert_score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from matplotlib->bert_score) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from matplotlib->bert_score) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from matplotlib->bert_score) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from matplotlib->bert_score) (3.2.1)\n",
      "Requirement already satisfied: six>=1.5 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.3)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: tabulate, portalocker, colorama, sacrebleu, evaluate\n",
      "Successfully installed colorama-0.4.6 evaluate-0.4.3 portalocker-3.1.1 sacrebleu-2.5.1 tabulate-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets evaluate bert_score nltk sacrebleu\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:finbot] *",
   "language": "python",
   "name": "conda-env-finbot-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
