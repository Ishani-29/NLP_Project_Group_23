{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05c66951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (4.52.0.dev0)\n",
      "Requirement already satisfied: datasets in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (2.21.0)\n",
      "Requirement already satisfied: peft in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (0.14.0)\n",
      "Requirement already satisfied: accelerate in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (1.6.0)\n",
      "Requirement already satisfied: bitsandbytes in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (0.45.5)\n",
      "Requirement already satisfied: huggingface_hub in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (0.30.2)\n",
      "Requirement already satisfied: filelock in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: psutil in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from peft) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from huggingface_hub) (4.13.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: networkx in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets peft accelerate bitsandbytes huggingface_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1307276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (4.52.0.dev0)\n",
      "Requirement already satisfied: filelock in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from requests->transformers) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "152ac3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.52.0.dev0\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: bert-score, kvpress, peft, sentence-transformers, trl, unsloth_zoo\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fee0f1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"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\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ab2f0e9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40411/2517292343.py:68: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1278' max='1278' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1278/1278 13:59, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>9.452700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.574800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.957800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.916200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.051200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.621800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.420800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.368500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.328700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.326600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.309100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.307300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.311100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.314900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.276100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.265800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.272400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.269700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.293000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.271900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.234600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.199600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.184000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.210500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.177700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.188000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.216800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.208100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.216300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.209100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.217400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.206700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.190500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.190100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.211800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.187600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.198800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.221000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.196100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.174200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.206100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.180100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.160500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.157400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.132700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.148100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.160600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.135700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.160700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.159500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.137400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.151200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.169800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.140600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.142300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.134100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.150500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.139800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.142700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.156400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>0.147600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.143200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>0.157800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1278, training_loss=0.5099634579462997, metrics={'train_runtime': 842.8294, 'train_samples_per_second': 3.029, 'train_steps_per_second': 1.516, 'total_flos': 6893622943285248.0, 'train_loss': 0.5099634579462997, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import LEDTokenizer, LEDForConditionalGeneration, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "\n",
    "# Configuration\n",
    "model_id = \"allenai/led-base-16384\"\n",
    "dataset_id = \"ishani29/mahakumbh-news-summarization\"\n",
    "split = \"train\"\n",
    "max_input_length = 4096  # LED supports up to 16,384\n",
    "max_target_length = 512\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = LEDTokenizer.from_pretrained(model_id)\n",
    "model = LEDForConditionalGeneration.from_pretrained(model_id).to(device)\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(dataset_id, split=split)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(example):\n",
    "    input_text = f\"{example['text']}\"\n",
    "    target_text = example['summary']\n",
    "\n",
    "    model_input = tokenizer(\n",
    "        input_text,\n",
    "        truncation=True,\n",
    "        max_length=max_input_length,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        label = tokenizer(\n",
    "            target_text,\n",
    "            truncation=True,\n",
    "            max_length=max_target_length,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "    model_input[\"labels\"] = label[\"input_ids\"]\n",
    "\n",
    "    # Set global attention on <s> token (first token)\n",
    "    model_input[\"global_attention_mask\"] = [1] + [0] * (len(model_input[\"input_ids\"]) - 1)\n",
    "\n",
    "    return model_input\n",
    "\n",
    "# Tokenize dataset\n",
    "tokenized = dataset.map(preprocess, remove_columns=dataset.column_names)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./led-news-summarizer\",\n",
    "    per_device_train_batch_size=1,  # larger input size ‚Üí smaller batch\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=3e-5,\n",
    "    fp16=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=20,\n",
    "    report_to=\"none\",\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "576d0f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f4b5158d0142ebb96d6f9b350fd096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/648M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68bf562eb64c44e2bb85412dd9f7b289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ishani29/led-mahakumbh/commit/fd53c540324139f68d3b2a89b0f7de8c45fec3fd', commit_message='Upload tokenizer', commit_description='', oid='fd53c540324139f68d3b2a89b0f7de8c45fec3fd', pr_url=None, repo_url=RepoUrl('https://huggingface.co/ishani29/led-mahakumbh', endpoint='https://huggingface.co', repo_type='model', repo_id='ishani29/led-mahakumbh'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"ishani29/led-mahakumbh\")\n",
    "tokenizer.push_to_hub(\"ishani29/led-mahakumbh\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9828f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (0.4.3)\n",
      "Requirement already satisfied: bert-score in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (0.3.13)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from evaluate) (2.21.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from evaluate) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from evaluate) (0.30.2)\n",
      "Requirement already satisfied: packaging in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from evaluate) (24.1)\n",
      "Requirement already satisfied: torch>=1.0.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from bert-score) (2.5.0)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from bert-score) (4.52.0.dev0)\n",
      "Requirement already satisfied: matplotlib in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from bert-score) (3.10.0)\n",
      "Requirement already satisfied: filelock in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
      "Requirement already satisfied: aiohttp in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.11.11)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.12.14)\n",
      "Requirement already satisfied: networkx in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (0.4.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from matplotlib->bert-score) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from matplotlib->bert-score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from matplotlib->bert-score) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from matplotlib->bert-score) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from matplotlib->bert-score) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from matplotlib->bert-score) (3.2.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->bert-score) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install evaluate bert-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "068ec0ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aed952dee8dd45adbb583e1229de63a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.22k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b76c0951c9004f94b7eb2f30f1b3d0f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/999k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12d195ba811b4acba2726f5ec27b93a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e91bd84d370f41ad97562dd100fcad87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/957 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a745d1bc5074805bba021a0872cbb04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.14k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a01cfa747fae4e85ac0a2352e224a4cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/648M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09e92312d8fd4303894fb58f155979ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 151/151 [02:25<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî∏ ROUGE Scores:\n",
      "rouge1: 0.5698\n",
      "rouge2: 0.3485\n",
      "rougeL: 0.4466\n",
      "\n",
      "üî∏ BLEU Score: 0.3173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî∏ BERTScore (F1): 0.9189\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import LEDTokenizer, LEDForConditionalGeneration\n",
    "from evaluate import load as load_metric\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "\n",
    "# Paths\n",
    "model_path = \"ishani29/led-mahakumbh\"\n",
    "dataset_id = \"ishani29/mahakumbh-news-summarization\"\n",
    "split = \"test\"\n",
    "output_file = \"generated_summaries_led.csv\"\n",
    "\n",
    "# Load model & tokenizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = LEDTokenizer.from_pretrained(model_path)\n",
    "model = LEDForConditionalGeneration.from_pretrained(model_path).to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load test dataset\n",
    "dataset = load_dataset(dataset_id, split=split)\n",
    "\n",
    "# Load metrics\n",
    "rouge = load_metric(\"rouge\")\n",
    "bleu = load_metric(\"bleu\")\n",
    "bertscore = load_metric(\"bertscore\")\n",
    "\n",
    "# Generate summaries\n",
    "generated_summaries = []\n",
    "reference_summaries = []\n",
    "\n",
    "for example in tqdm(dataset, desc=\"Evaluating\"):\n",
    "    input_text = example[\"text\"]\n",
    "    reference = example[\"summary\"]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=4096  # LED supports long sequences\n",
    "    ).to(device)\n",
    "\n",
    "    # Set global attention on <s> token\n",
    "    global_attention_mask = torch.zeros_like(inputs[\"input_ids\"])\n",
    "    global_attention_mask[:, 0] = 1  # global attention on first token\n",
    "\n",
    "    with torch.no_grad():\n",
    "        summary_ids = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            global_attention_mask=global_attention_mask,\n",
    "            max_length=150,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    decoded_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    generated_summaries.append(decoded_summary)\n",
    "    reference_summaries.append(reference)\n",
    "\n",
    "# Save to CSV\n",
    "with open(output_file, mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Generated Summary\", \"Reference Summary\"])\n",
    "    for gen_summary, ref_summary in zip(generated_summaries, reference_summaries):\n",
    "        writer.writerow([gen_summary, ref_summary])\n",
    "\n",
    "# Evaluate ROUGE\n",
    "rouge_result = rouge.compute(predictions=generated_summaries, references=reference_summaries)\n",
    "print(\"\\nüî∏ ROUGE Scores:\")\n",
    "for key in [\"rouge1\", \"rouge2\", \"rougeL\"]:\n",
    "    print(f\"{key}: {rouge_result[key]:.4f}\")\n",
    "\n",
    "# Evaluate BLEU\n",
    "tokenized_preds = [' '.join(pred.split()) for pred in generated_summaries]\n",
    "tokenized_refs = [[' '.join(ref.split())] for ref in reference_summaries]\n",
    "bleu_result = bleu.compute(predictions=tokenized_preds, references=tokenized_refs)\n",
    "print(f\"\\nüî∏ BLEU Score: {bleu_result['bleu']:.4f}\")\n",
    "\n",
    "# Evaluate BERTScore\n",
    "bertscore_result = bertscore.compute(predictions=generated_summaries,\n",
    "                                     references=reference_summaries,\n",
    "                                     lang=\"en\")\n",
    "bert_f1 = sum(bertscore_result[\"f1\"]) / len(bertscore_result[\"f1\"])\n",
    "print(f\"\\nüî∏ BERTScore (F1): {bert_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88954841",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /mnt/Data/sarmistha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /mnt/Data/sarmistha/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /mnt/Data/sarmistha/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Computing BLEU...\n",
      "üîç Computing ROUGE...\n",
      "üîç Computing METEOR...\n",
      "üîç Computing BERTScore...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Evaluation Summary:\n",
      "BLEU: 0.3173\n",
      "ROUGE-1: 0.5698\n",
      "ROUGE-2: 0.3485\n",
      "ROUGE-L: 0.4466\n",
      "METEOR: 0.5045\n",
      "BERTScore_F1: 0.9189\n",
      "\n",
      "‚úÖ Evaluation metrics saved to: 'generated_summaries_led_metrics.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# ----- CONFIG -----\n",
    "csv_input_path = \"generated_summaries_led.csv\"  # Change if your CSV has a different name\n",
    "eval_output_path = \"generated_summaries_led_metrics.csv\"\n",
    "model_id = \"ishani29/led-mahakumbh\"  # Replace with your Gemma model checkpoint ID if calculating perplexity\n",
    "\n",
    "# ----- LOAD DATA -----\n",
    "df = pd.read_csv(csv_input_path)\n",
    "references = df[\"Reference Summary\"].tolist()\n",
    "predictions = df[\"Generated Summary\"].tolist()\n",
    "# articles = df[\"article\"].tolist()\n",
    "\n",
    "# ----- EVALUATION METRICS -----\n",
    "bleu = evaluate.load(\"sacrebleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "meteor = evaluate.load(\"meteor\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "print(\"üîç Computing BLEU...\")\n",
    "bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "\n",
    "print(\"üîç Computing ROUGE...\")\n",
    "rouge_score = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"üîç Computing METEOR...\")\n",
    "meteor_score = meteor.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"üîç Computing BERTScore...\")\n",
    "bert_score = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "\n",
    "# # ----- (OPTIONAL) PERPLEXITY -----\n",
    "# try:\n",
    "#     print(\"üîç Calculating Perplexity (optional)...\")\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "#     model = AutoModelForSeq2SeqLM.from_pretrained(model_id).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     model.eval()\n",
    "\n",
    "#     perplexities = []\n",
    "#     for article, reference in tqdm(zip(articles, references), total=len(articles), desc=\"Calculating Perplexity\"):\n",
    "#         inputs = tokenizer(article, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n",
    "#         labels = tokenizer(reference, return_tensors=\"pt\", truncation=True, max_length=150).input_ids.to(model.device)\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             loss = model(input_ids=inputs.input_ids, labels=labels).loss\n",
    "#             perplexities.append(torch.exp(loss).item())\n",
    "\n",
    "#     df[\"perplexity\"] = perplexities\n",
    "#     avg_perplexity = sum(perplexities) / len(perplexities)\n",
    "# except Exception as e:\n",
    "#     print(f\"‚ö†Ô∏è Perplexity skipped due to error: {e}\")\n",
    "#     avg_perplexity = None\n",
    "\n",
    "# ----- SAVE METRICS -----\n",
    "metrics = {\n",
    "    \"BLEU\": bleu_score[\"score\"] / 100,\n",
    "    \"ROUGE-1\": rouge_score[\"rouge1\"],\n",
    "    \"ROUGE-2\": rouge_score[\"rouge2\"],\n",
    "    \"ROUGE-L\": rouge_score[\"rougeL\"],\n",
    "    \"METEOR\": meteor_score[\"meteor\"],\n",
    "    \"BERTScore_F1\": sum(bert_score[\"f1\"]) / len(bert_score[\"f1\"]),\n",
    "#     \"Avg Perplexity\": avg_perplexity if avg_perplexity is not None else \"N/A\"\n",
    "}\n",
    "\n",
    "pd.DataFrame([metrics]).to_csv(eval_output_path, index=False)\n",
    "\n",
    "# ----- PRINT METRICS -----\n",
    "print(\"\\nüìä Evaluation Summary:\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
    "print(f\"\\n‚úÖ Evaluation metrics saved to: '{eval_output_path}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:finbot] *",
   "language": "python",
   "name": "conda-env-finbot-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
