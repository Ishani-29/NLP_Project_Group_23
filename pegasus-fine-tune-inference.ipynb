{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05c66951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (4.52.0.dev0)\n",
      "Requirement already satisfied: datasets in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (2.21.0)\n",
      "Requirement already satisfied: peft in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (0.14.0)\n",
      "Requirement already satisfied: accelerate in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (1.6.0)\n",
      "Requirement already satisfied: bitsandbytes in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (0.45.5)\n",
      "Requirement already satisfied: huggingface_hub in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (0.30.2)\n",
      "Requirement already satisfied: filelock in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: psutil in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from peft) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from huggingface_hub) (4.13.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: networkx in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets peft accelerate bitsandbytes huggingface_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1307276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (4.52.0.dev0)\n",
      "Requirement already satisfied: filelock in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from requests->transformers) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "152ac3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.52.0.dev0\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: bert-score, kvpress, peft, sentence-transformers, trl, unsloth_zoo\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fee0f1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"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\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ab2f0e9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_32495/2796695114.py:75: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1278' max='1278' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1278/1278 05:27, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>6.855400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>5.916300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.889400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>5.708800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.811200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>5.326900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>5.061100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>4.840100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>5.289000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.606600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>4.670300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>4.781100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>4.417100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>4.400500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>4.836900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>4.953900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>4.666900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>4.314700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>4.743200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>4.530200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>4.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>4.369500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>4.549200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>3.941000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.406300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>3.905100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>3.871500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>3.639600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>3.530500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.247500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>2.990400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>3.181400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>2.763000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>2.535900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.311200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>2.224400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>1.916800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>1.952300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>1.638400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.508200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>1.496500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>1.577800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>1.346500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>1.280800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.080400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>1.343800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>1.193400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>1.263000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>1.316800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.316200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>1.052100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>1.145300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>1.340200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>1.015900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.073000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.980100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>1.170600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>1.017400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.976000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.014800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>1.198900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>1.009100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>1.118400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages/transformers/modeling_utils.py:3298: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 128, 'min_length': 32, 'num_beams': 8, 'length_penalty': 0.8}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1278, training_loss=2.990325885945829, metrics={'train_runtime': 329.1338, 'train_samples_per_second': 7.757, 'train_steps_per_second': 3.883, 'total_flos': 3688401324736512.0, 'train_loss': 2.990325885945829, 'epoch': 3.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Configuration\n",
    "model_id = \"google/pegasus-cnn_dailymail\"\n",
    "dataset_id = \"ishani29/mahakumbh-news-summarization\"\n",
    "split = \"train\"\n",
    "max_input_length = 512\n",
    "max_target_length = 150\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_id)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_id).to(device)\n",
    "\n",
    "# Optional: Apply PEFT with LoRA\n",
    "# peft_config = LoraConfig(\n",
    "#     task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "#     inference_mode=False,\n",
    "#     r=8,\n",
    "#     lora_alpha=16,\n",
    "#     lora_dropout=0.1,\n",
    "#     target_modules=[\"q\", \"v\"]  # optional - PEGASUS uses \"q\", \"v\" layers\n",
    "# )\n",
    "# model = get_peft_model(model, peft_config)\n",
    "# model.print_trainable_parameters()\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(dataset_id, split=split)\n",
    "\n",
    "# Preprocessing\n",
    "def preprocess(example):\n",
    "    input_text = f\"Summarize this article:\\n{example['text']}\"\n",
    "    target_text = example['summary']\n",
    "\n",
    "    model_input = tokenizer(\n",
    "        input_text,\n",
    "        truncation=True,\n",
    "        max_length=max_input_length,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        label = tokenizer(\n",
    "            target_text,\n",
    "            truncation=True,\n",
    "            max_length=max_target_length,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "    model_input[\"labels\"] = label[\"input_ids\"]\n",
    "    return model_input\n",
    "\n",
    "tokenized = dataset.map(preprocess, remove_columns=dataset.column_names)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./pegasus-news-lora\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=3e-5,\n",
    "    fp16=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=20,\n",
    "    report_to=\"none\",\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "576d0f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9321ad01318416ba1c2022dd63fe8db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.28G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b7c10b22f314d5a9f6d5dba0d1990e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c46838b487f04baba06d04057f02d7c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ishani29/pegasus-mahakumbh/commit/5a86a5ec0ca6dc7fd0120c4e456c020ca543a921', commit_message='Upload tokenizer', commit_description='', oid='5a86a5ec0ca6dc7fd0120c4e456c020ca543a921', pr_url=None, repo_url=RepoUrl('https://huggingface.co/ishani29/pegasus-mahakumbh', endpoint='https://huggingface.co', repo_type='model', repo_id='ishani29/pegasus-mahakumbh'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"ishani29/pegasus-mahakumbh\")\n",
    "tokenizer.push_to_hub(\"ishani29/pegasus-mahakumbh\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9828f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (0.4.3)\n",
      "Requirement already satisfied: bert-score in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (0.3.13)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from evaluate) (2.21.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from evaluate) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from evaluate) (0.30.2)\n",
      "Requirement already satisfied: packaging in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from evaluate) (24.1)\n",
      "Requirement already satisfied: torch>=1.0.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from bert-score) (2.5.0)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from bert-score) (4.52.0.dev0)\n",
      "Requirement already satisfied: matplotlib in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from bert-score) (3.10.0)\n",
      "Requirement already satisfied: filelock in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
      "Requirement already satisfied: aiohttp in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.11.11)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.12.14)\n",
      "Requirement already satisfied: networkx in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (0.4.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from matplotlib->bert-score) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from matplotlib->bert-score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from matplotlib->bert-score) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from matplotlib->bert-score) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from matplotlib->bert-score) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from matplotlib->bert-score) (3.2.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/Data/sarmistha/.miniconda3/envs/finbot/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->bert-score) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install evaluate bert-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "068ec0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 151/151 [04:23<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔸 ROUGE Scores:\n",
      "rouge1: 0.5142\n",
      "rouge2: 0.2893\n",
      "rougeL: 0.4002\n",
      "\n",
      "🔸 BLEU Score: 0.2392\n",
      "\n",
      "🔸 BLEU Score: 0.2392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔸 BERTScore (F1): 0.9086\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "from evaluate import load as load_metric\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "\n",
    "# Paths\n",
    "model_path = \"ishani29/pegasus-mahakumbh\"  # or \"ishani29/pegasus-mahakumbh-lora\"\n",
    "dataset_id = \"ishani29/mahakumbh-news-summarization\"\n",
    "split = \"test\"\n",
    "output_file = \"generated_summaries_pegasus_1.csv\"\n",
    "\n",
    "# Load model & tokenizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_path)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_path).to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load test dataset\n",
    "dataset = load_dataset(dataset_id, split=split)\n",
    "\n",
    "# Load metrics\n",
    "rouge = load_metric(\"rouge\")\n",
    "bleu = load_metric(\"bleu\")\n",
    "bertscore = load_metric(\"bertscore\")\n",
    "\n",
    "# Generate summaries\n",
    "generated_summaries = []\n",
    "reference_summaries = []\n",
    "\n",
    "for example in tqdm(dataset, desc=\"Evaluating\"):\n",
    "    input_text = example[\"text\"]\n",
    "    reference = example[\"summary\"]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        summary_ids = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=150,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    decoded_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    generated_summaries.append(decoded_summary)\n",
    "    reference_summaries.append(reference)\n",
    "\n",
    "# Save the generated summaries and references to a CSV file\n",
    "with open(output_file, mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Generated Summary\", \"Reference Summary\"])  # header\n",
    "    for gen_summary, ref_summary in zip(generated_summaries, reference_summaries):\n",
    "        writer.writerow([gen_summary, ref_summary])\n",
    "\n",
    "# Evaluate ROUGE\n",
    "rouge_result = rouge.compute(predictions=generated_summaries, references=reference_summaries)\n",
    "print(\"\\n🔸 ROUGE Scores:\")\n",
    "for key in [\"rouge1\", \"rouge2\", \"rougeL\"]:\n",
    "    print(f\"{key}: {rouge_result[key]:.4f}\")\n",
    "\n",
    "# Evaluate BLEU (BLEU expects tokenized predictions and list of tokenized references)\n",
    "# tokenized_preds = [pred.split() for pred in generated_summaries]\n",
    "# tokenized_refs = [[ref.split()] for ref in reference_summaries]\n",
    "# Assuming `generated_summaries` and `reference_summaries` are your raw summaries.\n",
    "# Generate tokenized summaries\n",
    "tokenized_preds = [' '.join(pred.split()) for pred in generated_summaries]\n",
    "tokenized_refs = [[' '.join(ref.split())] for ref in reference_summaries]\n",
    "\n",
    "# Now evaluate BLEU\n",
    "bleu_result = bleu.compute(predictions=tokenized_preds, references=tokenized_refs)\n",
    "print(f\"\\n🔸 BLEU Score: {bleu_result['bleu']:.4f}\")\n",
    "\n",
    "bleu_result = bleu.compute(predictions=tokenized_preds, references=tokenized_refs)\n",
    "print(f\"\\n🔸 BLEU Score: {bleu_result['bleu']:.4f}\")\n",
    "\n",
    "# Evaluate BERTScore\n",
    "bertscore_result = bertscore.compute(predictions=generated_summaries,\n",
    "                                     references=reference_summaries,\n",
    "                                     lang=\"en\")\n",
    "bert_f1 = sum(bertscore_result[\"f1\"]) / len(bertscore_result[\"f1\"])\n",
    "print(f\"\\n🔸 BERTScore (F1): {bert_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88954841",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /mnt/Data/sarmistha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /mnt/Data/sarmistha/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /mnt/Data/sarmistha/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Computing BLEU...\n",
      "🔍 Computing ROUGE...\n",
      "🔍 Computing METEOR...\n",
      "🔍 Computing BERTScore...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Evaluation Summary:\n",
      "BLEU: 0.2392\n",
      "ROUGE-1: 0.5142\n",
      "ROUGE-2: 0.2893\n",
      "ROUGE-L: 0.4002\n",
      "METEOR: 0.4208\n",
      "BERTScore_F1: 0.9086\n",
      "\n",
      "✅ Evaluation metrics saved to: 'mahakumbh_eval_metrics_pegasus_1.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# ----- CONFIG -----\n",
    "csv_input_path = \"generated_summaries_pegasus_1.csv\"  # Change if your CSV has a different name\n",
    "eval_output_path = \"mahakumbh_eval_metrics_pegasus_1.csv\"\n",
    "model_id = \"ishani29/pegasus-mahakumbh\"  # Replace with your Gemma model checkpoint ID if calculating perplexity\n",
    "\n",
    "# ----- LOAD DATA -----\n",
    "df = pd.read_csv(csv_input_path)\n",
    "references = df[\"Reference Summary\"].tolist()\n",
    "predictions = df[\"Generated Summary\"].tolist()\n",
    "# articles = df[\"article\"].tolist()\n",
    "\n",
    "# ----- EVALUATION METRICS -----\n",
    "bleu = evaluate.load(\"sacrebleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "meteor = evaluate.load(\"meteor\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "print(\"🔍 Computing BLEU...\")\n",
    "bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "\n",
    "print(\"🔍 Computing ROUGE...\")\n",
    "rouge_score = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"🔍 Computing METEOR...\")\n",
    "meteor_score = meteor.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"🔍 Computing BERTScore...\")\n",
    "bert_score = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "\n",
    "# # ----- (OPTIONAL) PERPLEXITY -----\n",
    "# try:\n",
    "#     print(\"🔍 Calculating Perplexity (optional)...\")\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "#     model = AutoModelForSeq2SeqLM.from_pretrained(model_id).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     model.eval()\n",
    "\n",
    "#     perplexities = []\n",
    "#     for article, reference in tqdm(zip(articles, references), total=len(articles), desc=\"Calculating Perplexity\"):\n",
    "#         inputs = tokenizer(article, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n",
    "#         labels = tokenizer(reference, return_tensors=\"pt\", truncation=True, max_length=150).input_ids.to(model.device)\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             loss = model(input_ids=inputs.input_ids, labels=labels).loss\n",
    "#             perplexities.append(torch.exp(loss).item())\n",
    "\n",
    "#     df[\"perplexity\"] = perplexities\n",
    "#     avg_perplexity = sum(perplexities) / len(perplexities)\n",
    "# except Exception as e:\n",
    "#     print(f\"⚠️ Perplexity skipped due to error: {e}\")\n",
    "#     avg_perplexity = None\n",
    "\n",
    "# ----- SAVE METRICS -----\n",
    "metrics = {\n",
    "    \"BLEU\": bleu_score[\"score\"] / 100,\n",
    "    \"ROUGE-1\": rouge_score[\"rouge1\"],\n",
    "    \"ROUGE-2\": rouge_score[\"rouge2\"],\n",
    "    \"ROUGE-L\": rouge_score[\"rougeL\"],\n",
    "    \"METEOR\": meteor_score[\"meteor\"],\n",
    "    \"BERTScore_F1\": sum(bert_score[\"f1\"]) / len(bert_score[\"f1\"]),\n",
    "#     \"Avg Perplexity\": avg_perplexity if avg_perplexity is not None else \"N/A\"\n",
    "}\n",
    "\n",
    "pd.DataFrame([metrics]).to_csv(eval_output_path, index=False)\n",
    "\n",
    "# ----- PRINT METRICS -----\n",
    "print(\"\\n📊 Evaluation Summary:\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
    "print(f\"\\n✅ Evaluation metrics saved to: '{eval_output_path}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:finbot] *",
   "language": "python",
   "name": "conda-env-finbot-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
